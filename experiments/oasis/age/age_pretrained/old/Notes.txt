Comment: the effects of re-training the full model after only re-training the final layer are very interesting. 
The MAE's suddenly jump a lot! Plot the training curves in your dissertation!

Exp 1: 2.90 MAE (got expected result)
Exp 2: Train MAE 2.2 (really low) but Valid MAE (2.9-3.0)
--> increase regularization with experiment 5 and 6

Exp 3: 2.90 MAE (but difference between train and valid MAE is zero!)
Exp 4: 2.90 MAE (not really an improvement!)
Exp 5 (increase weight decay compared to 2): 3.14 MAE (after achieving 2.90 MAE with re-training final layer, very weird)
- this run illustrates overfitting quite well!
Exp 6 (increase dropout compared to 2): Similar to Exp 5
Exp 7 (increase dropout and weight decay): 4.75 MAE
Exp 8 (almost no decline for small learning rate): Generally, quite good results (smooth continueation of training but no improvement) (but really good test
results!)
Exp 9 (almost no decline for small learning rate and smaller batch size): Similar to exp 8 (also quite good test results!)
----------------

We build on Exp8!

Final two optimization experiments before running finals:

Exp 10 (increase weight decay compared to Exp 8): 2.96 validation 
Exp 11 (increase dropout compared to Exp 9): 3.0 MAE 

Exp 12 (increase weighte decay compared to Exp 8): 2.77-2.82 valid MAE TAKE THIS AS FINAL!
Exp 13 (increase learning rate compared to Exp 8): 2.92 valid MAE

Exp 14 (replicating Exp 12 with more epochs and higher epoch_dec since it did not seem to have converged): 2.83 valid MAE: massively overfitting 
Exp 15 (increase regularization compared to 15): 2.83 MAE
Exp 16 (increase regularization compared to 16): 2.83 MAE

AFTER THIS: PERFORM FINAL EXPERIMENTS!