A,B: 
- Test accuracy goes up to 0.80 which is quite high but validation accuracy is at 0.56
- I am not sure whether it has actually converged. Potentially training longer? Since we 
change tasks we might need more epochs to train than same-task transfer.
- Moreover, the huge gap in performance between training and validation data seems 
to indicate overfitting.


C: test accuracy goes up to 0.85 and increase in validation accuracy to 0.68
-increase the number of epochs to train the model

D: test accuracy 0.98 and validation accuracy 0.82
- Increase regularization effect: increas weight decay compared to A.
- Avoid pre-training of final layer. That seems to be detrimental. Features might not be useful.

E: 0.83
- compare with D with pre-training of final layer
--> slight improvement but not significant

F: 0.86
- increase regularization of D --> regularization seems to bring improvement

G: 0.45
- increase regularization signficantly --> bad idea, worsen results signficantly 

H: 0.78
- increase regularization compared to F 

---DROPOUT
I: 0.85
- increase dropout compared to F: 0.7 

J: 0.7
- increase dropout compared to F:  0.9

K: 0.85
- increase dropout compared to F: 0.6 


---RETRAIN FINAL LAYERS 
Run L, N, M
L: 0.75
-re-train final 2 layers.

M: 0.916 accuracy 
- re-train final 3 layers 
- increase dropout to 0.6
- increase weight decay to 5e-2 

N: 0.72 accuracy
-re-train final 4 layers.

--> weird result: 
re-initialization of the final 3 layers 
seem to be best but re-initialization of the final
2 layers seems to worsen results.

O: 

P:

Q: 


