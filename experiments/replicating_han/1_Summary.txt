A: 2.93 (Han)
B: 2.93 (trained longer)
C: 2.9 (trained longer)
D: 3.3 (MAE loss! Detrimental!)
E: 2.91 (same as A)
F: 2.93 (train longer)
G: 2.91 (overfitted but went worse, changing schedule parameters)
H: 2.91 ()
I: 2.91 (higher weight decay, less epochs, maybe try out with more)
J: 2.91 (higher gamma)
K: 2.89 (small batch size)
L: 2.77 (higher weight decay, higher learning rate)
M: 2.94 (high gamma (0.9))
O: NA
P: 3.91 (batch size set to 2)
Q: 2.92 (higher weight decay and less epochs)
R: 2.93 (batch=4,wd=1e-3,lr=1e-3)
S: 2.93  (lower momentum)
T: 3.12 (higher weight decay, weird result.)
U: 2.83 (different combination of lr, wdec and gamma)
V: 2.94 (Change to K: Decrease weight decay for last layer, increase patience.)
W: 
X:
Y:
Z: